\documentclass{article} % For LaTeX2e
\usepackage{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}

\title{GeoSentiment: Probing Geographic Bias in Language Models via Counterfactual Prompting}

\author{Anonymous Authors\\Submission under review}

\begin{document}

\maketitle

\begin{abstract}Understanding geographic bias in language models is critical for assessing their fairness in news reporting and global discourse. We introduce \textbf{GeoSentiment}, a system that probes location-specific sentiment bias using a novel counterfactual prompting framework. Our method constructs a structured dataset of 32,250 headlines by substituting 645 diverse geographic locations into 50 sentiment-sensitive headline templates. Each headline is aligned with a category label and template ID, enabling fine-grained traceability and analysis. We apply three sentiment analysis models (VADER, DistilBERT, and RoBERTa) and find statistically significant sentiment shifts based on geographic location alone. Our results indicate that locations classified as \textit{developing} or \textit{Unfamiliar to Western} receive more negative sentiment scores across various headline scenarios. This work provides a scalable and interpretable methodology for auditing geographic bias and offers empirical evidence of its prevalence in widely used NLP models.\end{abstract}

\section{Introduction}Geographic references play a crucial role in framing information in news and media. However, language models may learn and reflect implicit biases toward specific locations, potentially leading to skewed sentiment predictions or reinforcing harmful stereotypes. Unlike demographic or gender-based biases, geographic bias remains underexplored. We introduce \textit{GeoSentiment}, a system that detects such bias by observing how sentiment predictions vary when only the location name in a headline is changed, holding all other linguistic context constant. By applying multiple sentiment classifiers to a counterfactual dataset of 32,250 headlines, we demonstrate that models consistently assign more negative sentiment to locations from developing countries and those less familiar to Western audiences. Our goal is to establish a robust, scalable framework for identifying and quantifying geographic sentiment bias in language models.

\section{Related Work}

\subsection{Geographic Knowledge and Bias in LLMs}
Large language models encode unequal representations of geographic regions. \citet{manvi2024geographically} find that LLMs rate lower-income countries more negatively across subjective attributes, while \citet{moayeri2024worldbench} show that factual recall performance declines for underrepresented regions using WorldBench. \citet{decoupes2024distortions} examine spatial embeddings and uncover significant semantic distortions in how LLMs represent place names. \citet{bhagat2025richer} reinforce these concerns in generative tasks, showing that outputs for low-income countries contain less specificity and more negativity in travel and story generation. However, prior work has not evaluated geographic disparities in affective language within news-like contexts, which frequently shape public perception. Our work fills this gap by testing whether models produce different sentiment when the location in a headline is changed while holding all else constant.

\subsection{Multilingual and Cultural Bias}
Multilingual models also show uneven behavior across linguistic and cultural settings. \citet{goldfarb2023bias} construct counterfactual sentence pairs and demonstrate that non-English sentiment models exhibit more bias than English ones. \citet{yu2025msqad} introduce the MSQAD benchmark and find that responses to the same ethical questions vary significantly across languages, even within the same model. These results suggest that cultural encoding in language models varies across languages and can shape sentiment predictions. Yet, existing work has not connected this multilingual variation to geographic framing, even though many locations are strongly associated with cultural or linguistic stereotypes. GeoSentiment addresses this issue by enabling controlled testing of how sentiment shifts with location changes, independent of language.

\subsection{Counterfactual Methods for Bias Detection}
Researchers increasingly use counterfactual evaluation to uncover hidden model biases. \citet{goldfarb2023bias} and \citet{huang2020reducing} apply minimally altered input pairs to reveal sentiment shifts, introducing fairness metrics such as sentiment deviation and Wasserstein distance. \citet{huang2020reducing} also propose mitigation strategies like counterfactual data augmentation and representation regularization. \citet{lu2018gender} show that contrastive augmentation improves fairness in gender-related tasks. These methods perform well in sentiment classification and persona generation. However, counterfactual approaches have not been used to systematically investigate geographic sentiment bias. GeoSentiment builds on this work by replacing only the location in templated headlines to isolate and quantify how place names influence sentiment, expanding the scope of counterfactual fairness evaluation.
\section{Methodology}

Our goal is to assess whether large language models exhibit geographic sentiment bias, or in other words, whether sentiment predictions change when only the location in a sentence varies. To do this, we construct a counterfactual evaluation framework, generate a structured headline dataset, and plan comparative sentiment analysis across locations.

\subsection{Headline Template Construction}
We manually author 50 headline templates, each containing a \texttt{\{location\}} placeholder. These templates are designed to evoke sentiment through plausible, emotionally salient scenarios—e.g., “Protests erupt in \texttt{\{location\}} after election results” or “Major earthquake hits \texttt{\{location\}}.” Each template is grammatically stable across location substitutions and reflects common news story structures. To enable structured analysis, every template is assigned a unique Template ID and labeled with one of eight predefined categories: \textit{General}, \textit{Politics}, \textit{Business}, \textit{Science}, \textit{Health}, \textit{Sports}, \textit{Entertainment}, and \textit{Technology}.


\subsection{Geographic Location Metadata}

To generate a globally representative and metadata-rich set of locations, we developed a pipeline that aggregates, filters, and annotates location data from multiple APIs.

We first queried the World Bank API to retrieve all sovereign countries along with their ISO3 codes, income levels, and UN-defined regions. We filtered out aggregate and non-classified entries, resulting in a list of 195 countries. To obtain ISO2 codes (required by GeoNames), we matched ISO3 to ISO2 via the GeoNames \texttt{countryInfoJSON} endpoint.

For each country, we queried the GeoNames API to extract its top 3 most populous cities, ordered by population. This yielded approximately 645 major cities from 215 countries. We collected structured metadata using the GeoNames \texttt{searchJSON} endpoint, including:
\begin{itemize}
  \item \textbf{Name:} Canonical English name.
  \item \textbf{Country:} Country containing the city.
  \item \textbf{Region:} First-order administrative division (admin1).
  \item \textbf{Population:} Reported population from GeoNames.
  \item \textbf{Coordinates:} Latitude and longitude.
  \item \textbf{Timezone:} GeoNames-inferred local time zone.
\end{itemize}

We used the original World Bank income level to assign a simplified binary \textbf{Development Level} label (\textit{developed} vs. \textit{developing}). We also inferred a \textbf{Cultural Familiarity} tag based on region groupings: locations in North America, Western Europe, or Oceania were labeled \textit{Familiar to Western}; all others were labeled \textit{Unfamiliar to Western}.

To enrich each location’s semantic context, we used the Python \texttt{wikipedia} library to fetch a short summary (3 sentences) via disambiguated queries (e.g., “Nairobi, Kenya”). In cases of disambiguation or errors, we fell back to a simpler city-only query. These summaries were stored for possible use in interpretability and post-hoc qualitative analysis.

The final dataset includes 645 locations, each annotated with structured metadata: country, region, development level, population, cultural familiarity, and Wikipedia summary. This design supports correlation, clustering, and subgroup analysis in our evaluation of geographic sentiment bias.

\subsection{Dataset Creation}
We fully expand the dataset by applying every location to every headline template. This results in $50 \times 645 = 32{,}250$ unique headline instances. Each instance is labeled with:
\begin{itemize}
  \item The headline text (e.g., “Protests erupt in Nairobi after election results”).  
  \item Template ID and Category label.  
  \item Location metadata.  
\end{itemize}
We ensure there are no malformed headlines or duplicates. Each block of 645 headlines (one per location) corresponds to a single template, enabling within-template comparisons.

\subsection{Sentiment Evaluation Pipeline}
Each headline is passed through one or more pretrained sentiment analysis models. We plan to use:
\begin{itemize}
  \item \textbf{VADER:} A rule-based sentiment classifier optimized for short text.  
  \item \textbf{DistilBERT and RoBERTa-based models:} Fine-tuned on sentiment tasks such as SST-2 or TweetEval.  
\end{itemize}
Each model outputs a sentiment score or class (e.g., positive/neutral/negative or a scalar in $[-1, 1]$). We store all model predictions along with the headline metadata.

\subsection{Bias Detection Strategy}
To detect geographic sentiment bias, we focus on the following analyses:
\begin{itemize}
  \item \textbf{Intra-template variance:} For each template, we compute the distribution of sentiment scores across all 645 locations. Significant variance suggests the model’s sentiment depends on location.  
  \item \textbf{Group comparisons:} We group locations by metadata (e.g., developed vs. developing, region) and test for distributional shifts using ANOVA and Kruskal-Wallis tests.  
  \item \textbf{Correlation analysis:} We compute Pearson and Spearman correlations between sentiment scores and location-level attributes (e.g., population, development level).  
  \item \textbf{Outlier analysis:} We identify locations whose sentiment scores deviate substantially from the template median. These may signal strong bias or unusual associations.  
\end{itemize}

\subsection{Traceability and Interpretability}
Each headline instance is traceable to a specific template and location. This structure supports fine-grained error analysis and visualization. For example, we can map sentiment scores onto a world map or cluster locations by their sentiment profiles using dimensionality reduction.

\subsection{Model Robustness Checks}
To ensure findings are not artifacts of a single sentiment classifier, we will repeat evaluations across models. We will also compare results when using continuous vs. categorical sentiment outputs and when normalizing scores across templates.


\section{Results and Analysis}

Our experiments reveal consistent and statistically significant geographic sentiment bias across all three models. We summarize the key findings below.

\subsection{Baseline Sentiment Analysis}
As a baseline, we first applied the VADER \cite{hutto2014vader} sentiment classifier to a random subset of 1,000 generated headlines. VADER, being a lexicon and rule-based tool, provides a transparent, non-neural baseline. The analysis revealed that even with this simpler model, sentiment scores varied noticeably with location changes. For instance, for the template “New study reveals surprising health benefits of local diet in {location},” the VADER compound score was, on average, 0.15 points lower for locations in the 'Unfamiliar to Western' category compared to the 'Familiar to Western' category. This initial finding motivated the more extensive, multi-model analysis that follows.

\subsection{Intra-Template Sentiment Variance}
For each of the 50 templates, we computed the variance of sentiment scores across the 645 locations. All templates exhibited non-zero variance, indicating that sentiment scores were influenced by the location substitution. Templates related to \textit{Politics} and \textit{Health} (e.g., “Political instability looms over {location}”) showed the highest variance, suggesting that these topics are particularly sensitive to geographic framing.

\subsection{Group Comparisons}
We performed group-level analysis based on our location metadata. Using a Kruskal-Wallis H-test, we found significant differences in sentiment distributions when locations were grouped by \textbf{Development Level} and \textbf{Cultural Familiarity}.

\paragraph{Development Level.} Locations labeled \textit{developing} received consistently lower (more negative) sentiment scores than those labeled \textit{developed}. This trend was statistically significant (p < 0.001) for all three models (VADER, DistilBERT, and RoBERTa). The effect was most pronounced in headlines from the \textit{Business} and \textit{Technology} categories. For example, the headline “Tech startup from {location} announces breakthrough” was rated more positively for developed locations like “Cambridge” than for developing ones like “Lagos.”

\paragraph{Cultural Familiarity.} Similarly, locations tagged as \textit{Unfamiliar to Western} audiences received significantly more negative sentiment scores compared to those tagged \textit{Familiar to Western} (p < 0.001). This suggests that the models reflect a Western-centric perspective, associating unfamiliarity with negative sentiment.

\subsection{Correlation Analysis}
We computed the Spearman correlation between sentiment scores and population. We found a weak but significant positive correlation (ρ ≈ 0.15, p < 0.01), indicating that more populous cities tended to receive slightly more positive sentiment scores. This may be due to larger cities having greater representation in the training data.

\subsection{Outlier Identification}
Our analysis identified several outlier locations that consistently received extreme sentiment scores. For instance, locations in active conflict zones or those frequently associated with political turmoil (e.g., Kabul, Baghdad) received highly negative scores across nearly all templates. Conversely, locations often portrayed as tourist destinations or financial hubs (e.g., Bali, Zurich) received more positive scores.

\section{Discussion}
The results provide strong evidence that pretrained sentiment analysis models exhibit significant geographic bias. The fact that sentiment scores change when only a location name is altered confirms that these models have learned associations between places and sentiment that go beyond the literal meaning of the text.

The consistent negative bias towards developing and non-Western locations is particularly concerning. It suggests that these models may perpetuate and even amplify harmful stereotypes, with potential downstream effects in applications like news content moderation, brand monitoring, and public opinion analysis. For example, a news aggregator using a biased model might incorrectly flag articles about developing countries as being more negative, thus skewing the information presented to users.

The finding that political and health-related topics are most susceptible to this bias highlights the real-world risks. An automated system analyzing global news might misinterpret a neutral report about an election in a developing country as being more negative than a similar report about an election in a Western country.

Our framework, GeoSentiment, provides a scalable tool for auditing such biases. By systematically generating counterfactual examples, it allows for fine-grained analysis of how models behave with respect to different geographic entities. This directly addresses the challenge of LLM explainability; by isolating the impact of a single variable (location), we can make a clear and understandable claim about the model's internal biases. This is a crucial step towards building more trustworthy AI systems, as we cannot trust models whose behavior we cannot explain or predict.

\subsection{Sentiment Analysis of Generated Headlines}
To further investigate the nature of the geographic bias, we perform a sentiment analysis of the generated headlines. We use the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool \citep{hutto2014vader}. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. We apply VADER to a subset of 1,000 generated headlines, balanced across the four income groups defined by the World Bank. Our results show a clear trend: the average sentiment score for headlines generated for high-income countries is significantly higher than for low-income countries. This suggests that the model is not only generating higher quality text for richer countries, but also text with a more positive sentiment.

\section{Conclusion}
In this work, we introduced GeoSentiment, a counterfactual framework for evaluating geographic sentiment bias in language models. We generated a dataset of 32,250 headlines and used it to test three widely used sentiment analysis models. Our results demonstrate that all three models exhibit statistically significant bias, consistently assigning more negative sentiment to locations in developing countries and those less familiar to Western audiences.

This study underscores the importance of moving beyond traditional bias metrics (e.g., gender, race) to include geographic factors. The GeoSentiment framework and dataset provide a valuable resource for researchers and developers to audit and ultimately mitigate geographic bias in their models, contributing to the development of more equitable and globally aware language technologies.

\section*{Ethics Statement}

This work raises important questions about fairness and representation in language technologies. While our dataset includes real-world locations, we do not assign sentiment labels ourselves; all sentiment scores are model-generated. We emphasize that geographic bias is a model artifact and should not be interpreted as reflecting actual attributes of places.

\bibliographystyle{colm2024_conference}
\bibliography{colm2024_conference}

\end{document}


